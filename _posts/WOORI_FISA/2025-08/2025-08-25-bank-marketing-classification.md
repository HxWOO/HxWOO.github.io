---
layout: post
title:  "🏦 은행 고객은 왜 떠났을까? 분류 모델링 A to Z"
date:   2025-08-25 12:00:00 +0900
categories: [WOORI_FISA, Machine Learning]
tags: [분류, 데이터전처리, 하이퍼파라미터튜닝, AutoML, PyCaret, Scikit-learn, '#우리FIS아카데미', '#우리FISA', '#AI엔지니어링', '#K-디지털트레이닝', '#우리에프아이에스', '#글로벌소프트웨어캠퍼스']
---

<br>

## :tropical_drink: 시작하며: 현실적인 모델링에 도전하다

이번에는 은행 마케팅 데이터를 가지고 고객이 정기 예금에 가입할지 예측하는 프로젝트를 진행했다. 데이터 전처리부터 여러 모델을 비교하고, 하이퍼파라미터 튜닝, 그리고 AutoML 도구인 PyCaret까지 사용해본, 그야말로 머신러닝 분류의 종합선물세트 같은 경험이었다. 

> #### 💡 "진짜 중요한 건, 실제 예금 고객을 놓치지 않는 것!"
> 이번 프로젝트의 핵심 목표는 바로 **재현율(Recall)** 을 높이는 것이었다. 모델의 정확도만 높이는 건 의미가 없었다. 진짜 고객을 하나라도 더 찾아내는 모델을 만드는 여정, 지금 시작한다.

---

### 📥 1단계: 데이터와의 첫 만남

UCI 저장소의 [은행 마케팅 데이터](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)를 사용했다. 고객의 나이, 직업, 대출 여부 같은 정보로 예금 가입 여부(`y`)를 예측하는 문제였다. 데이터를 살펴보니 `no`와 `yes` 비율이 약 8:1로, **데이터 불균형**이 심각했다. 이건 정확도만 믿고 가다간 큰일난다는 신호였다. 🚨

그리고 중요한 발견! `duration`(통화 시간)이라는 피처가 있었는데, 이건 예측 시점에서는 알 수 없는, 결과에 가까운 정보였다. 이런 사후 정보를 사용하면 모델이 과대적합되기 쉬워서 과감히 제외했다. 현실적인 모델링의 첫걸음이었다고 생각한다.

### ✨ 2단계: 데이터 성형수술 (전처리)

모델이 데이터를 잘 소화할 수 있도록 깔끔하게 다듬어주는 과정이 필수였다.

-   **수치형 데이터**: 피처마다 값의 범위가 제각각이라 **StandardScaler**를 사용해서 평균 0, 분산 1로 맞춰줬다. 이상치에 덜 민감해서 가장 무난하고 안정적인 선택이었다.
-   **범주형 데이터**: 문자열은 모델이 이해 못 하니까 숫자로 바꿔야 했다. 각 카테고리를 새로운 피처로 만드는 **OneHotEncoder**를 사용했다. `drop='first'` 옵션으로 다중공선성 문제도 피하고 피처 수도 줄이는 꿀팁도 잊지 않았다.

```python
# 수치형 데이터 스케일링
scaler = StandardScaler()
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])

# 범주형 데이터 인코딩
encoder = OneHotEncoder(drop='first')
# ... 인코딩 과정 생략 ...
```

---

### ⚙️ 3단계: 모델 쇼핑! 최고의 분류기는?

의사결정 나무, 랜덤 포레스트, KNN, 로지스틱 회귀, 이렇게 4가지 기본 모델을 만들어서 성능을 비교해봤다. 이번 문제에서는 **재현율(Recall)** 이 가장 중요한 지표였기 때문에, 이 점수를 중점적으로 봤다.

> **결과는?** 전반적인 성능(정확도, AUC)은 **랜덤 포레스트**가 가장 좋았지만, 정작 중요한 재현율은 다른 모델과 비슷하거나 오히려 낮기도 했다. 모델마다 장단점이 있다는 걸 다시 한번 느꼈다.

### 🛠️ 4단계: 모델 성능 쥐어짜기 (하이퍼파라미터 튜닝)

가장 유력했던 랜덤 포레스트를 `RandomizedSearchCV`로 최적화해봤다. 모든 조합을 다 해보는 `GridSearchCV`보다 훨씬 효율적이기 때문이다.

```python
# RandomizedSearchCV로 최적 파라미터 탐색
param_grid = {'n_estimators': [100, 300, 500], 'max_depth': [50, 80, 100], ...}
rfc = RandomForestClassifier(random_state=42)
rm_model = RandomizedSearchCV(rfc, param_distributions=param_grid, cv=5, scoring='recall', n_iter=5)
rm_model.fit(X_train, y_train)
```

> **그런데...** 튜닝 후 재현율이 오히려 떨어졌다. 😭 무조건 튜닝이 능사는 아니라는 뼈아픈 교훈을 얻었다. 기본 모델이 이미 괜찮았거나, 내가 설정한 파라미터 범위가 최적이 아니었을 수도 있겠다는 생각이 들었다.

---

### 🤖 5단계: AutoML 치트키, PyCaret 맛보기

몇 줄의 코드로 이 모든 과정을 자동화하는 PyCaret을 써봤는데, 이건 정말 신세계였다!

```python
from pycaret.classification import *

s = setup(df_bank_ready2, target = 'y', session_id = 123)
best = compare_models()
tuned_best_model = tune_model(best)
```

> 데이터 전처리부터 모델 비교, 튜닝까지 알아서 다 해주니 생산성이 폭발했다. 프로젝트 초기에 빠르게 베이스라인 모델을 만들 때 앞으로 무조건 사용할 것 같다.

---

### ✨ 오늘의 회고

이번 프로젝트는 머신러닝 분류 문제의 A to Z를 경험해본 소중한 시간이었다. **데이터 불균형**의 중요성을 몸으로 느꼈고, 상황에 맞는 **전처리**와 **평가 지표(특히 재현율!)** 선택이 얼마나 중요한지 깨달았다.

여러 모델을 직접 만들고 비교하며 각 알고리즘의 장단점을 파악했고, **하이퍼파라미터 튜닝**이 항상 정답은 아니라는 현실적인 교훈도 얻었다. 마지막으로 **PyCaret** 같은 AutoML 도구의 강력함을 맛보면서, 앞으로는 더 빠르고 효율적으로 모델링을 시작할 수 있겠다는 자신감이 생겼다.