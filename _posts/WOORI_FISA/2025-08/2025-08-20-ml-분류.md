---
layout: post
title:  "🧠 머신러닝의 근간, 퍼셉트론과 아달린으로 분류의 세계 맛보기"
date:   2025-08-20 21:00:00 +0900
categories: [Machine Learning, Woori FISA]
tags: ['머신러닝', '선형분류기', '퍼셉트론', '아달린', '경사하강법', '#우리FIS아카데미', '#우리FISA', '#AI엔지니어링', '#K-디지털트레이닝', '#우리에프아이에스', '#글로벌소프트웨어캠퍼스']
---

<br>

## 🧠 오늘의 항해: 머신러닝의 바다로 떠나다

오늘은 머신러닝의 근간이 되는 **분류(Classification) 알고리즘**을 만나보는 시간을 가졌다. 특히 인공지능의 시초라고 할 수 있는 **퍼셉트론(Perceptron)** 과 그 진화형인 **아달린(ADALINE)** 의 세계로 떠나, 이들이 어떻게 동작하고 학습하는지 직접 코드를 짜보며 깊이 이해해보았다. 이 알고리즘들은 우리가 흔히 아는 딥러닝의 뿌리이기도 하다. 오늘의 학습은 마치 머신러닝이라는 바다의 첫 항해를 떠나는 기분이었다.

> ### :guard: "분류는 왜 이렇게 중요한가요?"
> 머신러닝의 핵심 작업 중 하나인 분류는, 주어진 데이터를 '이것' 혹은 '저것'으로 나누는 작업이다. 이메일이 스팸인지 아닌지, 종양이 양성인지 악성인지 판별하는 것 모두가 바로 '분류'이다. 이는 산업 전반에 걸쳐 활용되고 있으니, 그 중요성은 말할 것도 없다.

---

### 🧮 인공 뉴런: 퍼셉트론과 아달린

#### 수학적 정의
- 퍼셉트론과 아달린 모두 **입력 신호의 가중합**을 계산한다.
- 수식: $ z = w_1x_1 + w_2x_2 + \cdots + w_mx_m = \boldsymbol{w}^T\boldsymbol{x} $
  - $\boldsymbol{w}$: 가중치 벡터
  - $\boldsymbol{x}$: 입력 벡터
  - $z$: 최종 입력 (net input)

#### 결정 함수 (퍼셉트론)
- 퍼셉트론은 **계단 함수**를 사용하여 최종 출력을 결정한다.
- 수식: $\phi(z)=\begin{cases}1 & z \ge 0 \\ -1 & \text{그 외} \end{cases}$
- 이 함수는 최종 입력 $z$가 임계값 0 이상이면 1, 아니면 -1을 출력한다. 마치 '스위치'처럼 동작하죠!

#### 활성화 함수 (아달린)
- 아달린은 연속적인 값을 갖는 **선형 활성화 함수**를 사용한다.
- 수식: $\phi(z) = z$
- 이는 최종 입력 $z$를 그대로 출력한다. 이 연속적인 값을 기준으로 클래스를 나눈다.

#### 학습 규칙 (퍼셉트론)
- 퍼셉트론의 학습 규칙은 다음과 같다.
- 수식: $\Delta w_j = \eta(y^{(i)} - \hat{y}^{(i)})x_j^{(i)}$
  - $\eta$: 학습률
  - $y^{(i)}$: 실제 값
  - $\hat{y}^{(i)}$: 예측 값
  - $x_j^{(i)}$: $j$번째 특성의 $i$번째 샘플 값
- 이 규칙은 **예측이 틀렸을 때만** 가중치를 업데이트하여 학습한다.

#### 비용 함수 (아달린)
- 아달린은 연속적인 출력을 사용하므로, 연속적인 오차를 줄이는 방식으로 학습한다.
- 이에 사용되는 것이 **비용 함수 (Cost Function)** 이다.
- 수식: $J(\boldsymbol{w}) = \dfrac{1}{2}\sum_i\left(y^{(i)} - \phi(z^{(i)})\right)^2$
- 이 함수는 실제 값과 예측 값의 차이의 제곱합으로, 이 값을 **최소화**하는 방향으로 학습한다.

| 구분 | **퍼셉트론** | **아달린** |
| :--- | :--- | :--- |
| **활성화 함수** | 계단 함수 (비연속) | 선형 함수 (연속) |
| **학습 방법** | 오분류 시 가중치 업데이트 | 비용 함수 최소화 (경사 하강법) |
| **출력** | 1 또는 -1 | 연속적인 실수 값 |

---

### 🐍 파이썬으로 퍼셉트론 학습 알고리즘 구현

#### 객체 지향 퍼셉트론 API

```python
class Perceptron:
    def __init__(self, eta=0.001, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target - self.predict(xi))
                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0)
            self.errors_.append(errors)
        return self

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, -1)
```

#### 붓꽃 데이터셋에서 퍼셉트론 훈련

- `Iris-setosa`와 `Iris-versicolor` 두 클래스를 선택하여 이진 분류 문제로 만들었다.
- `sepal length`와 `petal length` 두 가지 특성만 사용했다.
- 퍼셉트론 모델을 훈련시킨 후, 결정 경계(Decision Boundary)를 시각화했다.
- 데이터가 선형 분리 가능했기 때문에, 퍼셉트론은 잘 작동했다. 🎉

| 코드 | 설명 | 특징 |
| :--- | :--- | :--- |
| **`ppn = Perceptron()`** | 퍼셉트론 모델을 생성합니다. | 학습률, 에포크 수 등을 지정할 수 있습니다. |
| **`ppn.fit(X, y)`** | 모델을 훈련합니다. | 훈련 과정에서 오류가 줄어드는지 확인할 수 있습니다. |
| **`ppn.predict(X_new)`** | 새로운 데이터에 대해 예측합니다. | 1 또는 -1을 반환합니다. |

---

### 📈 적응형 선형 뉴런 (ADALINE)과 학습의 수렴

#### 경사 하강법 (Gradient Descent)

- 아달린은 연속적인 오차를 최소화하기 위해 **경사 하강법**을 사용한다.
- **비용 함수** $J(\boldsymbol{w})$의 기울기(Gradient)를 따라 가중치 $\boldsymbol{w}$를 업데이트한다.
- 수식: $\boldsymbol{w} := \boldsymbol{w} + \Delta \boldsymbol{w}$, where $\Delta \boldsymbol{w} = \eta \sum_i (y^{(i)} - \phi(z^{(i)}))x^{(i)}$

#### 파이썬으로 아달린 구현

```python
class AdalineGD:
    def __init__(self, eta=0.001, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            cost = (errors**2).sum() / 2.0
            self.cost_.append(cost)
        return self

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, X):
        return X # 선형 활성화

    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)
```

#### 특성 스케일 조정 (Feature Scaling)

- 경사 하강법은 입력 특성의 **스케일**에 매우 민감하다.
- 특성 간 스케일이 차이가 크면 학습이 느려지거나 수렴하지 않을 수 있다.
- 이를 해결하기 위해 **표준화 (Standardization)** 를 사용한다.
- 수식: $x'_j = \dfrac{x_j - \mu_j}{\sigma_j}$
  - $\mu_j$: $j$번째 특성의 평균
  - $\sigma_j$: $j$번째 특성의 표준 편차

> 데이터의 스케일을 맞춰주는 표준화는 정말 중요하다! 이걸 하지 않으면 경사 하강법이 제대로 수렴하지 않을 수도 있어요. 😱

#### 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

- 배치 경사 하강법은 모든 훈련 샘플을 사용해 한 번의 업데이트를 수행한다.
- **확률적 경사 하강법**은 한 번에 하나의 샘플만 사용해 업데이트를 수행한다.
- 이는 더 빠르고, 대용량 데이터에 적합하며, 온라인 학습(실시간 학습)도 가능하게 한다.

```python
class AdalineSGD:
    # ... (위 코드 참조)
    def _update_weights(self, xi, target):
        """SGD 학습 규칙을 적용하여 가중치를 업데이트합니다"""
        output = self.activation(self.net_input(xi))
        error = (target - output)
        self.w_[1:] += self.eta * xi.dot(error)
        self.w_[0] += self.eta * error
        cost = 0.5 * error**2
        return cost
```

---

### ✨ 오늘의 회고

오늘은 머신러닝의 시초인 퍼셉트론과 아달린을 직접 구현해보며, 그 원리를 깊이 이해할 수 있었다. 퍼셉트론의 단순하지만 강력한 아이디어, 그리고 아달린의 연속적인 학습 방식과 경사 하강법의 활용까지, 머신러닝의 핵심 개념들을 짚어보는 값진 시간이었다. 특히, 특성 스케일 조정의 중요성을 직접 코드로 확인하면서 '이게 왜 필요한 거지?' 싶었던 의문이 깔끔하게 풀렸다. 😊

다음 학습에서는 이 알고리즘들을 기반으로 한 더 복잡한 분류기, 예를 들어 로지스틱 회귀나 서포트 벡터 머신(SVM) 등도 접해볼 예정인데, 오늘의 기초가 탄탄해서 기대가 된다! 더 많은 데이터와 알고리즘을 다루다 보면 언젠가 직접 딥러닝 모델을 만들 수 있는 날도 올 것 같다! 🧨