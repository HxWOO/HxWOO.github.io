---
layout: post
title: "[프로젝트 회고] 🤖 OpenAI와 GitHub Actions로 나만의 기업 분석 비서 만들기"
date: 2025-08-06 00:00:00 +0900
categories: [Project, Woori FISA]
tags: [python, project, openai, gpt, github-actions, automation, web-scraping, '#우리FIS아카데미', '#우리FISA', '#AI엔지니어링', '#K-디지털트레이닝', '#우리에프아이에스', '#글로벌소프트웨어캠퍼스']
---

> "매일 아침, 최고의 기업 분석 보고서가 내 GitHub에 자동으로 쌓인다면?"

<br>

## 🚀 나만의 AI 기업 분석 비서, 왜 만들었을까?

취준생의 가장 큰 숙제 중 하나는 바로 **기업 분석**...
하지만 수많은 기업의 기술 스택, 레거시, 미래 비전까지 깊이 있게 파악하기란
여간 어려운 일이 아니었다 😥

그래서 결심했다. **"나만의 `AI 기업 분석 비서`를 만들자!"**

이 프로젝트는 **Python**으로 코스피 상위 기업 목록을 슥- 가져오고,
**OpenAI GPT-4o API**를 활용해 기업을 심층 분석하며,
그 결과를 IT 신입 지원자 맞춤형 **Word 보고서**로 뚝딱 만들어낸다.

그리고 이 모든 과정을 **GitHub Actions**를 통해
매일 자동으로 실행하여 내 GitHub 저장소에 차곡차곡 쌓아주는,
그런 완전 자동화 파이프라인이다 😊

---

### ✨ 그래서, 뭘 할 수 있는데?

| 기능 | 설명 |
| :--- | :--- |
| **📈 기업 목록 자동 수집** | `scraper.py`가 매일 네이버 금융에 출근해서 코스피 시총 100대 기업 목록을 최신으로 업데이트한다. |
| **🧠 AI 기반 심층 분석** | `main.py`가 똑똑한 GPT-4o를 호출해서 기업의 과거, 현재, 미래를 5가지 핵심 주제로 깊이 있게 파고든다. |
| **📄 맞춤형 보고서 생성** | 분석 결과를 바탕으로 백엔드, 인프라, AI 엔지니어 등 직무별 맞춤형 Word 보고서를 생성! 핵심 키워드는 **굵게** 강조해서 가독성도 높였다. |
| **🤖 완전 자동화 파이프라인** | GitHub Actions가 매일 아침 8시에 자동으로 스크립트를 실행하고, 생성된 보고서를 GitHub에 커밋까지 해준다. 그야말로 꿈의 자동화! |

---

### 🛠️ 어떻게 만들었을까? (feat. 사용한 연장들)

#### 1. Python 스크립트: 정찰병과 브레인

-   **`scraper.py` (정찰병)**: 매일 네이버 금융에 방문해 코스피 100대 기업 목록을 수집하고 `kospi_top_100.txt` 파일에 저장하는 듬직한 녀석이다.
-   **`main.py` (브레인)**: 이 프로젝트의 핵심! `progress.txt`를 보고 오늘 분석할 기업을 정하고, OpenAI API를 호출해 분석한 뒤, `python-docx`로 보고서를 만들어낸다.

#### 2. OpenAI GPT-4o: 전문 분석가 역할 부여

그냥 정보를 묻는 걸 넘어, AI에게 **"저명한 IT 산업 및 기술 전략 분석가"** 라는 역할을 부여해서 답변의 깊이를 더했다. 기업의 과거(Legacy), 현재(State), 미래(Future)를 연결하는 통찰력 있는 분석을 유도하니, 결과물의 퀄리티가 확 달라졌다.

```python
# main.py의 일부

system_prompt = """당신은 저명한 IT 산업 및 기술 전략 분석가입니다. ...
답변 내용 중 가장 중요한 핵심 키워드나 문장은 **굵은 글씨** 처리를 위해 양쪽을 **로 감싸주세요."""

user_prompt = f"'__{company_name}__'가 창립 이후 겪어온 주요 기술적 변곡점들은 무엇인가요? ..."
```

#### 3. GitHub Actions: 자동화의 심장 💖

이 프로젝트의 화룡점정은 바로 **GitHub Actions**다.
`.github/workflows/main.yml` 파일에 이 모든 자동화의 비밀이 담겨있다.

1.  **매일 아침 8시에 자동으로 실행** (`cron` 스케줄러)
2.  Python 환경 설정 및 필요한 라이브러리 설치
3.  `scraper.py` 실행 → 최신 기업 목록 업데이트
4.  `main.py` 실행 → 다음 기업 분석 및 보고서 생성 (GitHub Secrets에 저장된 `OPENAI_API_KEY` 사용)
5.  변경된 파일들을 자동으로 Git에 커밋 및 푸시

```yaml
# .github/workflows/main.yml

name: Daily Company Analysis Report
on:
  schedule:
    # 매일 한국 시간 오전 8시에 실행 (UTC 23:00)
    - cron: '0 23 * * *'
  workflow_dispatch: # 수동 실행도 가능

jobs:
  build-and-commit:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - uses: actions/setup-python@v4
    # ... (의존성 설치)
    - name: Run scraper & main script
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python scraper.py
        python main.py
    - name: Commit and push
      # ... (Git 설정 및 커밋/푸시)
```

---

### 📈 프로젝트는 이렇게 성장했다!

이 프로젝트는 하루아침에 만들어지지 않았다.
단순한 아이디어에서 시작해, 하나씩 살을 붙여나가며 지금의 모습을 갖추게 된 과정이다.

-   **Phase 1: 기본 아이디어** - "기업 정보 조회해서 Word로 만들자!"
-   **Phase 2: 목적 구체화** - "IT 신입 지원자에게 진짜 도움이 되는 내용으로 바꾸자!"
-   **Phase 3: 분석의 깊이** - "과거-현재-미래를 잇는 심층 분석을 추가하자!"
-   **Phase 4: 가독성 개선** - "중요한 키워드는 **굵게** 표시해서 눈에 잘 띄게 하자!"
-   **Phase 5: 완전 자동화** - "GitHub Actions로 매일 자동으로 돌아가게 만들자!" 🔥

---

## ✨ 회고를 마치며

단순한 파이썬 스크립트에서 시작해, AI API와 CI/CD 도구를 결합하여
**살아있는 데이터 파이프라인**을 직접 구축해본 경험은 정말 의미 있었다.
특히 매일 아침 내 GitHub 저장소에 새로운 분석 보고서가 자동으로 쌓이는 걸 볼 때마다
자동화의 강력함을 온몸으로 체감할 수 있었다.

이번 프로젝트를 통해 API 활용 능력뿐만 아니라,
명확한 목적을 가지고 점진적으로 기능을 개선해나가는 프로젝트 관리 능력도
함께 기를 수 있었던 것 같다.

앞으로는 채용공고와 연계해서 공고가 진행중인 기업을 선택하고 분석을 요청할 수 있는
인터랙티브 서비스로 발전시켜보고 싶다는 새로운 목표가 생겼다.
도전은 계속된다! 🚀
